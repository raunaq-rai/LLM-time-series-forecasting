{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raunaqrai/miniconda_x86_64/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/raunaqrai/miniconda_x86_64/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ Avg token count: 512\n",
      "‚öôÔ∏è  FLOPs per training step (batch): 6.75e+12\n",
      "üß™ Evaluation FLOPs: 1.69e+14\n",
      "üéØ Remaining budget for training: 9.98e+16\n",
      "üöÄ Max training steps allowed: 14781\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from compute_flops import estimate_max_training_steps\n",
    "\n",
    "max_steps = estimate_max_training_steps(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    lora_rank=2,\n",
    "    input_fraction=0.7,\n",
    "    batch_size=4,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=300,\n",
    "    context_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " üí° Total Training FLOPs: 6.76e+15\n",
      " üß™ Total Evaluation FLOPs: 1.69e+14\n",
      " üî¢ Total Combined FLOPs: 6.93e+15\n",
      " üéØ Percentage of FLOP budget used: 6.92927%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from compute_flops import compute_flops\n",
    "\n",
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    input_fraction=0.7,\n",
    "    lora_rank=8,\n",
    "    batch_size=4,\n",
    "    training_steps=1000,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=300,\n",
    "    context_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with 1000 time series\n",
    "- Split immediately ‚Üí 700 for training, 300 for evaluation\n",
    "- Tokenise the 700 training series into sequences\n",
    "- Train the model on this\n",
    "- Evaluate the model on the validation/test split (the 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>LoRA Rank</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Context Length</th>\n",
       "      <th>Max Steps</th>\n",
       "      <th>Average Validation Loss</th>\n",
       "      <th>Training FLOPs</th>\n",
       "      <th>Total FLOPs Used</th>\n",
       "      <th>Cumulative FLOPs</th>\n",
       "      <th>Exceeds Budget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a i</td>\n",
       "      <td>Untrained</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1926</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a ii</td>\n",
       "      <td>Default LoRA (4, 1e-5)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b i</td>\n",
       "      <td>LoRA (2, 1e-5)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b ii (same as a ii, so dont have to run it)</td>\n",
       "      <td>LoRA (4, 1e-5)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b iii</td>\n",
       "      <td>LoRA (8, 1e-5)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b iv</td>\n",
       "      <td>LoRA (2, 5e-5)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.7193</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>1.39e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b v</td>\n",
       "      <td>LoRA (4, 5e-5)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6628</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>2.08e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b vi</td>\n",
       "      <td>LoRA (8, 5e-5)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6244</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>2.77e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b vii</td>\n",
       "      <td>LoRA (2, 1e-4)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>2.77e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b viii</td>\n",
       "      <td>LoRA (4, 1e-4)</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.5934</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>3.46e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b ix</td>\n",
       "      <td>LoRA (8, 1e-4)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>512</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.5594</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>4.16e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b x</td>\n",
       "      <td>Best config (CL=128)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>128</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>4.85e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b xi</td>\n",
       "      <td>Best config (CL=768)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>768</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.3744</td>\n",
       "      <td>6.76e+15</td>\n",
       "      <td>6.93e+15</td>\n",
       "      <td>5.54e+16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Part              Experiment  \\\n",
       "0                                           a i               Untrained   \n",
       "1                                          a ii  Default LoRA (4, 1e-5)   \n",
       "2                                           b i          LoRA (2, 1e-5)   \n",
       "3   b ii (same as a ii, so dont have to run it)          LoRA (4, 1e-5)   \n",
       "4                                         b iii          LoRA (8, 1e-5)   \n",
       "5                                          b iv          LoRA (2, 5e-5)   \n",
       "6                                           b v          LoRA (4, 5e-5)   \n",
       "7                                          b vi          LoRA (8, 5e-5)   \n",
       "8                                         b vii          LoRA (2, 1e-4)   \n",
       "9                                        b viii          LoRA (4, 1e-4)   \n",
       "10                                         b ix          LoRA (8, 1e-4)   \n",
       "11                                          b x    Best config (CL=128)   \n",
       "12                                         b xi    Best config (CL=768)   \n",
       "\n",
       "    LoRA Rank  Learning Rate  Context Length  Max Steps  \\\n",
       "0           4        0.00001             512          0   \n",
       "1           4        0.00001             512       1000   \n",
       "2           2        0.00001             512          0   \n",
       "3           4        0.00001             512          0   \n",
       "4           8        0.00001             512          0   \n",
       "5           2        0.00005             512       1000   \n",
       "6           4        0.00005             512       1000   \n",
       "7           8        0.00005             512       1000   \n",
       "8           2        0.00010             512          0   \n",
       "9           4        0.00010             512       1000   \n",
       "10          8        0.00010             512       1000   \n",
       "11          8        0.00010             128       1000   \n",
       "12          8        0.00010             768       1000   \n",
       "\n",
       "    Average Validation Loss Training FLOPs Total FLOPs Used Cumulative FLOPs  \\\n",
       "0                    1.1926       0.00e+00         0.00e+00         0.00e+00   \n",
       "1                    0.8880       6.76e+15         6.93e+15         6.93e+15   \n",
       "2                    0.0000       0.00e+00         0.00e+00         6.93e+15   \n",
       "3                    0.8880       0.00e+00         0.00e+00         6.93e+15   \n",
       "4                    0.0000       0.00e+00         0.00e+00         6.93e+15   \n",
       "5                    0.7193       6.76e+15         6.93e+15         1.39e+16   \n",
       "6                    0.6628       6.76e+15         6.93e+15         2.08e+16   \n",
       "7                    0.6244       6.76e+15         6.93e+15         2.77e+16   \n",
       "8                    0.0000       0.00e+00         0.00e+00         2.77e+16   \n",
       "9                    0.5934       6.76e+15         6.93e+15         3.46e+16   \n",
       "10                   0.5594       6.76e+15         6.93e+15         4.16e+16   \n",
       "11                   0.6771       6.76e+15         6.93e+15         4.85e+16   \n",
       "12                   0.3744       6.76e+15         6.93e+15         5.54e+16   \n",
       "\n",
       "    Exceeds Budget  \n",
       "0            False  \n",
       "1            False  \n",
       "2            False  \n",
       "3            False  \n",
       "4            False  \n",
       "5            False  \n",
       "6            False  \n",
       "7            False  \n",
       "8            False  \n",
       "9            False  \n",
       "10           False  \n",
       "11           False  \n",
       "12           False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# FLOPs constants (from your setup)\n",
    "FLOPS_PER_STEP = 6.76e12\n",
    "EVAL_FLOPS = 1.69e14\n",
    "FLOP_BUDGET = 1e17\n",
    "\n",
    "# Define experiment blocks\n",
    "experiments = [\n",
    "    {\"Part\": \"a i\", \"Experiment\": \"Untrained\", \"LoRA Rank\": 4, \"Learning Rate\": 1e-5, \"Context Length\": 512, \"Max Steps\": 0, \"Average Validation Loss\":1.1926},\n",
    "    {\"Part\": \"a ii\", \"Experiment\": \"Default LoRA (4, 1e-5)\", \"LoRA Rank\": 4, \"Learning Rate\": 1e-5, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.8880},\n",
    "    {\"Part\": \"b i\", \"Experiment\": \"LoRA (2, 1e-5)\", \"LoRA Rank\": 2, \"Learning Rate\": 1e-5, \"Context Length\": 512, \"Max Steps\": 0, \"Average Validation Loss\":0},\n",
    "    {\"Part\": \"b ii (same as a ii, so dont have to run it)\", \"Experiment\": \"LoRA (4, 1e-5)\", \"LoRA Rank\": 4, \"Learning Rate\": 1e-5, \"Context Length\": 512, \"Max Steps\": 0, \"Average Validation Loss\":0.8880},\n",
    "    {\"Part\": \"b iii\", \"Experiment\": \"LoRA (8, 1e-5)\", \"LoRA Rank\": 8, \"Learning Rate\": 1e-5, \"Context Length\": 512, \"Max Steps\": 0, \"Average Validation Loss\":0},\n",
    "    {\"Part\": \"b iv\", \"Experiment\": \"LoRA (2, 5e-5)\", \"LoRA Rank\": 2, \"Learning Rate\": 5e-5, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.7193},\n",
    "    {\"Part\": \"b v\", \"Experiment\": \"LoRA (4, 5e-5)\", \"LoRA Rank\": 4, \"Learning Rate\": 5e-5, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.6628},\n",
    "    {\"Part\": \"b vi\", \"Experiment\": \"LoRA (8, 5e-5)\", \"LoRA Rank\": 8, \"Learning Rate\": 5e-5, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.6244},\n",
    "    {\"Part\": \"b vii\", \"Experiment\": \"LoRA (2, 1e-4)\", \"LoRA Rank\": 2, \"Learning Rate\": 1e-4, \"Context Length\": 512, \"Max Steps\": 0, \"Average Validation Loss\":0},\n",
    "    {\"Part\": \"b viii\", \"Experiment\": \"LoRA (4, 1e-4)\", \"LoRA Rank\": 4, \"Learning Rate\": 1e-4, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.5934},\n",
    "    {\"Part\": \"b ix\", \"Experiment\": \"LoRA (8, 1e-4)\", \"LoRA Rank\": 8, \"Learning Rate\": 1e-4, \"Context Length\": 512, \"Max Steps\": 1000, \"Average Validation Loss\":0.5594},\n",
    "    {\"Part\": \"b x\", \"Experiment\": \"Best config (CL=128)\", \"LoRA Rank\": 8, \"Learning Rate\": 1e-4, \"Context Length\": 128, \"Max Steps\": 1000, \"Average Validation Loss\":0.6771},\n",
    "    {\"Part\": \"b xi\", \"Experiment\": \"Best config (CL=768)\", \"LoRA Rank\": 8, \"Learning Rate\": 1e-4, \"Context Length\": 768, \"Max Steps\": 1000, \"Average Validation Loss\":0.3744}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(experiments)\n",
    "\n",
    "# Compute Training FLOPs\n",
    "df[\"Training FLOPs\"] = df[\"Max Steps\"] * FLOPS_PER_STEP\n",
    "\n",
    "# Compute Total FLOPs for each experiment (training + one-time evaluation, or 0 if no training)\n",
    "df[\"Total FLOPs Used\"] = df[\"Training FLOPs\"].apply(lambda x: 0 if x == 0 else x + EVAL_FLOPS)\n",
    "\n",
    "\n",
    "# Compute cumulative across rows (for budget planning)\n",
    "df[\"Cumulative FLOPs\"] = df[\"Total FLOPs Used\"].cumsum()\n",
    "\n",
    "# Flag if budget is exceeded\n",
    "df[\"Exceeds Budget\"] = df[\"Cumulative FLOPs\"] > FLOP_BUDGET\n",
    "\n",
    "# Formatting\n",
    "df[\"Training FLOPs\"] = df[\"Training FLOPs\"].apply(lambda x: f\"{x:.2e}\")\n",
    "df[\"Total FLOPs Used\"] = df[\"Total FLOPs Used\"].apply(lambda x: f\"{x:.2e}\")\n",
    "df[\"Cumulative FLOPs\"] = df[\"Cumulative FLOPs\"].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "# Show the table\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
