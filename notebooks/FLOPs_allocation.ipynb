{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the absolute path to the src directory\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from compute_flops import compute_flops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with 1000 time series\n",
    "- Split immediately → 700 for training, 300 for evaluation\n",
    "- Tokenise the 700 training series into sequences\n",
    "- Train the model on this\n",
    "- Evaluate the model on the validation/test split (the 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " 💡 Total Training FLOPs: 0.00e+00\n",
      " 🧪 Total Evaluation FLOPs: 1.69e+14\n",
      " 🔢 Total Combined FLOPs: 1.69e+14\n",
      " 🎯 Percentage of FLOP budget used: 0.16879%\n"
     ]
    }
   ],
   "source": [
    "from compute_flops import compute_flops\n",
    "\n",
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",  # your dataset\n",
    "    input_fraction=0.7,                     # 70% input, 30% to predict\n",
    "    lora_rank=0,                            # no LoRA => untrained\n",
    "    batch_size=1,                           # batch size irrelevant here\n",
    "    training_steps=0,                       # no training\n",
    "    flop_budget=1e17,                       # your FLOP budget\n",
    "    train_series_count=0,                   # not needed for untrained\n",
    "    eval_series_count=300,                 # evaluate all 300 test series\n",
    "    context_length=512                     # fixed context length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " 💡 Total Training FLOPs: 6.76e+15\n",
      " 🧪 Total Evaluation FLOPs: 1.69e+14\n",
      " 🔢 Total Combined FLOPs: 6.92e+15\n",
      " 🎯 Percentage of FLOP budget used: 6.92494%\n"
     ]
    }
   ],
   "source": [
    "from compute_flops import compute_flops\n",
    "\n",
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",  # or your full path\n",
    "    input_fraction=0.7,                     # 70% of each series used as input\n",
    "    lora_rank=4,                            # default LoRA rank\n",
    "    batch_size=4,                           # reasonable batch size\n",
    "    training_steps=1000,                    # only doing 1000 steps for now\n",
    "    flop_budget=1e17,                       # your FLOP budget\n",
    "    train_series_count=700,                 # 70% of 1000 series\n",
    "    eval_series_count=300,                  # evaluate on remaining 30%\n",
    "    context_length=512                      # fixed context length\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LoRA = 2 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " 💡 Total Training FLOPs: 6.75e+15\n",
      " 🧪 Total Evaluation FLOPs: 1.69e+14\n",
      " 🔢 Total Combined FLOPs: 6.92e+15\n",
      " 🎯 Percentage of FLOP budget used: 6.92277%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " 💡 Total Training FLOPs: 6.76e+15\n",
      " 🧪 Total Evaluation FLOPs: 1.69e+14\n",
      " 🔢 Total Combined FLOPs: 6.92e+15\n",
      " 🎯 Percentage of FLOP budget used: 6.92494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Context length used for FLOP estimation: 512 tokens\n",
      "\n",
      " 💡 Total Training FLOPs: 6.76e+15\n",
      " 🧪 Total Evaluation FLOPs: 1.69e+14\n",
      " 🔢 Total Combined FLOPs: 6.93e+15\n",
      " 🎯 Percentage of FLOP budget used: 6.92927%\n"
     ]
    }
   ],
   "source": [
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    input_fraction=0.7,\n",
    "    lora_rank=2,\n",
    "    batch_size=4,\n",
    "    training_steps=1000,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=300,\n",
    "    context_length=512\n",
    ")\n",
    "\n",
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    input_fraction=0.7,\n",
    "    lora_rank=4,\n",
    "    batch_size=4,\n",
    "    training_steps=1000,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=300,\n",
    "    context_length=512\n",
    ")\n",
    "\n",
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    input_fraction=0.7,\n",
    "    lora_rank=8,\n",
    "    batch_size=4,\n",
    "    training_steps=1000,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=300,\n",
    "    context_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should all be multipled by 3 since I will run 9 tests\n",
    "\n",
    "---\n",
    "\n",
    "Following this I will choose the best hyperparameters to vary context length from 128, 512, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
