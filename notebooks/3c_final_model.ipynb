{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "from preprocessor import LLMTIMEPreprocessor\n",
    "from load_qwen import load_qwen_model\n",
    "from final_model import LoRALinear, process_sequences, evaluate \n",
    "from compute_flops import compute_flops\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from final_model import train_lora_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, val_loader, val_loss, perplexity = train_lora_model(\n",
    "    max_steps=6000,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_flops(\n",
    "    data_path=\"../lotka_volterra_data.h5\",\n",
    "    input_fraction=1,\n",
    "    lora_rank=8,\n",
    "    batch_size=4,\n",
    "    training_steps=6000,\n",
    "    flop_budget=1e17,\n",
    "    train_series_count=700,\n",
    "    eval_series_count=200,\n",
    "    context_length=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trained_lora_model.pt\")\n",
    "print(\" Model weights saved to trained_lora_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# === Load base model and tokenizer ===\n",
    "model, tokenizer = load_qwen_model()\n",
    "\n",
    "lora_rank = 8  \n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank)\n",
    "    layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank)\n",
    "\n",
    "state_dict = torch.load(\"trained_lora_model.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Trained LoRA model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = LLMTIMEPreprocessor()\n",
    "with h5py.File(\"../lotka_volterra_data.h5\", \"r\") as f:\n",
    "    data = f[\"trajectories\"][:] \n",
    "\n",
    "def test_trained_model(data, sample_ids=[972], input_timesteps=70):\n",
    "    predictions = {}\n",
    "    for sample_id in sample_ids:\n",
    "        prey = data[sample_id, :, 0]\n",
    "        predator = data[sample_id, :, 1]\n",
    "\n",
    "        input_text, tokenized_input, scale_factor = preprocessor.preprocess_sample(prey, predator, input_timesteps)\n",
    "        tokens = tokenized_input.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(tokens, max_new_tokens=300)\n",
    "\n",
    "        semicolons = (generated[0] == tokenizer.convert_tokens_to_ids(';')).nonzero(as_tuple=True)[0]\n",
    "        while len(semicolons) < 100 and len(generated[0]) < 2000:\n",
    "            generated = model.generate(generated, max_new_tokens=20)\n",
    "            semicolons = (generated[0] == tokenizer.convert_tokens_to_ids(';')).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(semicolons) >= 100:\n",
    "            tokens_1d = generated[0][:semicolons[99] + 1]\n",
    "        else:\n",
    "            tokens_1d = generated[0]\n",
    "\n",
    "        decoded = tokenizer.decode(tokens_1d, skip_special_tokens=True)\n",
    "        decoded_pairs = [list(map(float, pair.split(','))) for pair in decoded.split(';') if ',' in pair]\n",
    "        decoded_prey, decoded_predator = zip(*decoded_pairs) if decoded_pairs else ([], [])\n",
    "\n",
    "        predictions[sample_id] = {\n",
    "            \"prey\": np.array(decoded_prey) * scale_factor,\n",
    "            \"predator\": np.array(decoded_predator) * scale_factor\n",
    "        }\n",
    "\n",
    "        print(f\" Sample {sample_id} | Input tokens: {tokenized_input.shape[1]}, Generated tokens: {generated.shape[1] - tokens.shape[1]}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# === Predict ===\n",
    "predictions = test_trained_model(data, sample_ids=[972], input_timesteps=50)\n",
    "\n",
    "# === Plotting function ===\n",
    "def plot_predictions(predictions, original_series, sample_id):\n",
    "    pred_prey = predictions[sample_id][\"prey\"]\n",
    "    pred_predator = predictions[sample_id][\"predator\"]\n",
    "    true_prey = original_series[:, 0]\n",
    "    true_predator = original_series[:, 1]\n",
    "    timesteps = np.arange(len(true_prey))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(timesteps, true_prey, label='True Prey', color='blue')\n",
    "    plt.plot(timesteps[:len(pred_prey)], pred_prey, '--', label='Predicted Prey', color='blue', alpha=0.6)\n",
    "    plt.title(f\"Prey - Sample {sample_id}\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(timesteps, true_predator, label='True Predator', color='red')\n",
    "    plt.plot(timesteps[:len(pred_predator)], pred_predator, '--', label='Predicted Predator', color='red', alpha=0.6)\n",
    "    plt.title(f\"Predator - Sample {sample_id}\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"sample{sample_id}_trained.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# === Run plotting and metrics ===\n",
    "original_series = data[972]\n",
    "plot_predictions(predictions, original_series, sample_id=972)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_metrics(predictions, original_series, sample_id):\n",
    "    pred_prey = predictions[sample_id][\"prey\"]\n",
    "    pred_predator = predictions[sample_id][\"predator\"]\n",
    "    true_prey = original_series[:, 0]\n",
    "    true_predator = original_series[:, 1]\n",
    "\n",
    "    # Ensure predictions and ground truth align in length\n",
    "    min_len_prey = min(len(pred_prey), len(true_prey))\n",
    "    min_len_predator = min(len(pred_predator), len(true_predator))\n",
    "\n",
    "    prey_metrics = {\n",
    "        \"MSE\": mean_squared_error(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "        \"MAE\": mean_absolute_error(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "        \"R²\":  r2_score(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "    }\n",
    "\n",
    "    predator_metrics = {\n",
    "        \"MSE\": mean_squared_error(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "        \"MAE\": mean_absolute_error(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "        \"R²\":  r2_score(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "    }\n",
    "\n",
    "    print(f\"\\nEvaluation Metrics for Sample ID {sample_id}\")\n",
    "    print(\"Prey:\")\n",
    "    for k, v in prey_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"Predator:\")\n",
    "    for k, v in predator_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    return {\"prey\": prey_metrics, \"predator\": predator_metrics}\n",
    "\n",
    "# === Run the metrics ===\n",
    "metrics = evaluate_metrics(predictions, original_series, sample_id=972)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# === Preprocessor and data ===\n",
    "preprocessor = LLMTIMEPreprocessor()\n",
    "with h5py.File(\"../lotka_volterra_data.h5\", \"r\") as f:\n",
    "    data = f[\"trajectories\"][:]  # (N, 100, 2)\n",
    "\n",
    "# === Prediction function for trained model ===\n",
    "def test_trained_model(data, sample_ids=[972], input_timesteps=70):\n",
    "    predictions = {}\n",
    "    for sample_id in sample_ids:\n",
    "        prey = data[sample_id, :, 0]\n",
    "        predator = data[sample_id, :, 1]\n",
    "\n",
    "        input_text, tokenized_input, scale_factor = preprocessor.preprocess_sample(prey, predator, input_timesteps)\n",
    "        tokens = tokenized_input.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(tokens, max_new_tokens=300)\n",
    "\n",
    "        semicolons = (generated[0] == tokenizer.convert_tokens_to_ids(';')).nonzero(as_tuple=True)[0]\n",
    "        while len(semicolons) < 100 and len(generated[0]) < 2000:\n",
    "            generated = model.generate(generated, max_new_tokens=20)\n",
    "            semicolons = (generated[0] == tokenizer.convert_tokens_to_ids(';')).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(semicolons) >= 100:\n",
    "            tokens_1d = generated[0][:semicolons[99] + 1]\n",
    "        else:\n",
    "            tokens_1d = generated[0]\n",
    "\n",
    "        decoded = tokenizer.decode(tokens_1d, skip_special_tokens=True)\n",
    "        decoded_pairs = [list(map(float, pair.split(','))) for pair in decoded.split(';') if ',' in pair]\n",
    "        decoded_prey, decoded_predator = zip(*decoded_pairs) if decoded_pairs else ([], [])\n",
    "\n",
    "        predictions[sample_id] = {\n",
    "            \"prey\": np.array(decoded_prey) * scale_factor,\n",
    "            \"predator\": np.array(decoded_predator) * scale_factor\n",
    "        }\n",
    "\n",
    "        print(f\" Sample {sample_id} | Input tokens: {tokenized_input.shape[1]}, Generated tokens: {generated.shape[1] - tokens.shape[1]}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def plot_predictions(predictions, original_series, sample_id):\n",
    "    pred_prey = predictions[sample_id][\"prey\"]\n",
    "    pred_predator = predictions[sample_id][\"predator\"]\n",
    "    true_prey = original_series[:, 0]\n",
    "    true_predator = original_series[:, 1]\n",
    "    timesteps = np.arange(len(true_prey))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(timesteps, true_prey, label='True Prey', color='blue')\n",
    "    plt.plot(timesteps[:len(pred_prey)], pred_prey, '--', label='Predicted Prey', color='blue', alpha=0.6)\n",
    "    plt.title(f\"Prey - Sample {sample_id}\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(timesteps, true_predator, label='True Predator', color='red')\n",
    "    plt.plot(timesteps[:len(pred_predator)], pred_predator, '--', label='Predicted Predator', color='red', alpha=0.6)\n",
    "    plt.title(f\"Predator - Sample {sample_id}\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_metrics(predictions, original_series, sample_id):\n",
    "    pred_prey = predictions[sample_id][\"prey\"]\n",
    "    pred_predator = predictions[sample_id][\"predator\"]\n",
    "    true_prey = original_series[:, 0]\n",
    "    true_predator = original_series[:, 1]\n",
    "\n",
    "    # Ensure predictions and ground truth align in length\n",
    "    min_len_prey = min(len(pred_prey), len(true_prey))\n",
    "    min_len_predator = min(len(pred_predator), len(true_predator))\n",
    "\n",
    "    prey_metrics = {\n",
    "        \"MSE\": mean_squared_error(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "        \"MAE\": mean_absolute_error(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "        \"R²\":  r2_score(true_prey[:min_len_prey], pred_prey[:min_len_prey]),\n",
    "    }\n",
    "\n",
    "    predator_metrics = {\n",
    "        \"MSE\": mean_squared_error(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "        \"MAE\": mean_absolute_error(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "        \"R²\":  r2_score(true_predator[:min_len_predator], pred_predator[:min_len_predator]),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Evaluation Metrics for Sample ID {sample_id}\")\n",
    "    print(\"Prey:\")\n",
    "    for k, v in prey_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"Predator:\")\n",
    "    for k, v in predator_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "    return {\"prey\": prey_metrics, \"predator\": predator_metrics}\n",
    "\n",
    "# === Predict for sample IDs 990–999 ===\n",
    "sample_ids = list(range(990, 1000))\n",
    "predictions = test_trained_model(data, sample_ids=sample_ids, input_timesteps=50)\n",
    "\n",
    "# === Track all metrics ===\n",
    "all_metrics = {\n",
    "    \"prey\": {\"MSE\": [], \"MAE\": [], \"R²\": []},\n",
    "    \"predator\": {\"MSE\": [], \"MAE\": [], \"R²\": []}\n",
    "}\n",
    "\n",
    "# === Evaluate and plot each ===\n",
    "for sample_id in sample_ids:\n",
    "    original_series = data[sample_id]\n",
    "    plot_predictions(predictions, original_series, sample_id)\n",
    "    metrics = evaluate_metrics(predictions, original_series, sample_id)\n",
    "\n",
    "    for target in [\"prey\", \"predator\"]:\n",
    "        for metric in [\"MSE\", \"MAE\", \"R²\"]:\n",
    "            all_metrics[target][metric].append(metrics[target][metric])\n",
    "\n",
    "# === Compute and print averages ===\n",
    "print(\"\\n Average Metrics Across Samples 990–999\")\n",
    "for target in [\"prey\", \"predator\"]:\n",
    "    print(f\"\\n {target.capitalize()}:\")\n",
    "    for metric in [\"MSE\", \"MAE\", \"R²\"]:\n",
    "        avg = np.mean(all_metrics[target][metric])\n",
    "        print(f\"  Avg {metric}: {avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
