\documentclass[a4paper,12pt]{article}
\usepackage{graphicx}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{amsmath}    % For math environments and symbols
\usepackage{amssymb}    % For \mathbb and other math symbols
\usepackage{amsfonts}   % Additional math fonts
\usepackage{float}
\setlength{\abovecaptionskip}{-2pt}  
\setlength{\belowcaptionskip}{1pt}
\usepackage{subcaption}
\usepackage[numbers]{natbib}
\usepackage{makecell}



% Title format
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}

% Title settings
\title{
    \includegraphics[scale=0.4]{Cam_logo_bw.png}\\
    \vspace{0.5cm}
    M2 Deep Learning - Coursework Assignment
}
\author{Raunaq Rai (rsr45@cam.ac.uk)\\
    Data Intensive Science, Department of Physics, University of Cambridge
}
\date{04 April, 2025 \\ \vspace{0.2cm} {\small Word Count: XXX}}

\begin{document}

\maketitle

\section*{Introduction}

Large Language Models have revolutionised natural language processing and have recently shown promise in domains beyond text, including time-series forecasting \citep{gruver2023language}. In this coursework, we explore the application of Low-Rank Adaptation (LoRA) \citep{hu2021lora} to the Qwen2.5-Instruct model \citep{qwen2.5}, a transformer-based LLM, for forecasting predator-prey population dynamics as described by the Lotka–Volterra equations \citep{takeuchi2006lotka}.

Building on the preprocessing methodology in the LLMTIME framework \citep{gruver2023language}, which adapts numeric sequences into a tokenizer-friendly format, we examine how Qwen2.5-Instruct can be repurposed for numerical prediction tasks. The objective is to fine-tune the model on a dataset of simulated predator-prey trajectories using LoRA, which allows for the adaptation of pretrained models without updating all weights.

Our experiments have a computational constraint: with a maximum allowable budget of $10^{17}$ floating point operations (FLOPS). The provided dataset consists of 1,000 synthetic time series representing prey and predator population dynamics, formatted as 100 time steps per sequence with two variables per step.

This report outlines the implementation of LLMTIME preprocessing, baseline evaluation of the untrained model, LoRA fine-tuning with hyperparameter sweeps, and analysis of forecasting performance under compute-efficient constraints. Our findings highlight the potential of adapting LLMs for scientific time-series forecasting.

\section{Compute Constraint}

This coursework promotes efficient experimentation under a strict compute budget of $10^{17}$ floating point operations (FLOPs) across all reported experiments. This constraint mirrors real-world limitations, and has encouraged us to be thoughtful and methodical with all experimentation.

\subsection*{FLOP Accounting Principles}

FLOP costs are calculated using simplified, hardware-agnostic assumptions. Table~\ref{tab:flops_primitives} provides the per-operation FLOP estimates. For example, multiplying an $m \times n$ matrix by an $n \times p$ matrix requires:
\begin{equation}
\text{FLOPs}_{\text{matmul}} = m \times p \times (2n - 1)
\end{equation}
We also assume that backpropagation incurs double the cost of the forward pass. This is because it is difficult to estimate the FLOPs for backpropagation accurately, as it depends on the implementation and optimisations used. However, a conservative estimate is to assume that the backward pass is approximately double the cost of the forward pass.
\begin{equation}
\text{FLOPs}_{\text{train}} = 3 \times \text{FLOPs}_{\text{forward}}
\end{equation}

\begin{table}[h]
  \centering
  \caption{FLOPs Accounting for Primitive Operations}
  \label{tab:flops_primitives}
  \begin{tabular}{lc}
    \hline
    Operation & FLOPs \\
    \hline
    Addition / Subtraction / Negation (float or int) & 1 \\
    Multiplication / Division / Inverse (float or int) & 1 \\
    ReLU / Absolute Value & 1 \\
    Exponentiation / Logarithm & 10 \\
    Sine / Cosine / Square Root & 10 \\
    \hline
  \end{tabular}
\end{table}

\subsection*{FLOP Estimator Implementation}

To estimate the computational cost of each experiment, we implemented a FLOP estimator in Python (\texttt{flops\_model.py} and \texttt{compute\_flops.py}). This estimator models a forward pass through the Qwen2.5-0.5B architecture at the operation level, breaking down FLOPs into different categories.

The estimator accounts for every trainable and non-trainable component of the model. For a given sequence length $n$, the estimator computes the total number of FLOPs for each of the following:

\begin{itemize}
  \item \textbf{Token Embeddings}: Retrieved via indexing and incur no arithmetic FLOPs.
  
  \item \textbf{Positional Embeddings}: Sinusoidal encodings are added to token embeddings, requiring $n \times d_{\text{model}}$ additions.

  \item \textbf{Multi-head Attention (per layer)}:
  \begin{itemize}
    \item Query, Key, and Value projections: Linear transformations to $d_{\text{head}}$ per head; includes multiplications and bias additions.
    \item Dot-product attention: Computation of $QK^\top$, scaling, softmax, and weighted sum with $V$.
    \item Softmax operation: Includes exponentiations, summations, and divisions.
    \item Output projection: Concatenation of heads followed by a linear transformation.
  \end{itemize}

  \item \textbf{Feedforward MLP with SwiGLU}: As described by \citet{shazeer2020glu}, this block consists of:
  \begin{itemize}
    \item Two parallel linear up-projections from the model dimension to the hidden dimension: one to produce the activation input, and one to produce the gating values.
    \item A \texttt{SwiGLU} activation function, which applies the SiLU nonlinearity to one stream and multiplies it elementwise with the other (gating).
    \item A final down-projection from the hidden dimension back to the model dimension.
  \end{itemize}  

  \item \textbf{RMSNorm Layers}: Following \citet{zhang2019root}, these involve:
  \begin{itemize}
    \item Elementwise squaring, summation, square root, division, and scaling by a learned parameter $\gamma$.
  \end{itemize}

  \item \textbf{LoRA Projections (if used)}: As proposed by \citet{hu2021lora}, LoRA introduces:
  \begin{itemize}
    \item Low-rank down- and up-projection matrices added to the frozen base weights.
    \item FLOPs from these include multiplications, additions, and a residual connection.
  \end{itemize}

  \item \textbf{Final Projection to Vocabulary Logits}:
  \begin{itemize}
    \item A linear layer projecting to the vocabulary space: $n \times d_{\text{model}} \times V$ multiplications and additions.
  \end{itemize}
\end{itemize}

These per-layer estimates are summed across all transformer blocks, and added to the overhead of input/output layers. FLOPs for different sequence lengths, LoRA ranks, and model dimensions are configurable through the function arguments.

\subsection*{Estimating Backward Pass Cost}

The exact computation of FLOPs for backpropagation is non-trivial and implementation-specific. In practice, the backward pass includes:

\begin{itemize}
  \item Derivatives for every parameter (including gradients for each tensor)
  \item Multiplications and additions for each chain rule application
  \item Memory reuse optimisations depending on the framework
\end{itemize}

For the purposes of this coursework, we assume that the backward pass is approximately double the cost of the forward pass. This is a conservative estimate, as it does not account for optimisations such as gradient checkpointing or memory reuse.


Thus, the total training FLOPs are given by:
\begin{equation}
\text{FLOPs}_{\text{train}} \approx 3 \times \text{FLOPs}_{\text{forward}}
\end{equation}


\section{LLMTIME Preprocessing Implementation}

We implemented the LLMTIME preprocessing scheme \citep{gruver2023language} to convert multivariate time-series data into a format suitable for Qwen2.5-Instruct. This was done by creating a dedicated Python module \texttt{src/preprocessor.py} containing a class \texttt{LLMTIMEPreprocessor}, which formats and tokenizes time-series data according to the scheme described in the coursework.

\subsection*{Scaling and Formatting}

Each sample consists of a pair of sequences—prey and predator population values over time. These values can vary significantly in magnitude both within and across samples, which can lead to inconsistent token lengths after formatting and impair the model’s ability to generalise.

To mitigate this, we compute a per-sample scaling factor $\alpha$ that normalises the numerical range before tokenisation:
\begin{equation}
\alpha = \frac{1}{10} \cdot \max\left(\text{percentile}_{95}(\text{prey}),\ \text{percentile}_{95}(\text{predator})\right)
\end{equation}

We then divide each value in the prey and predator sequences by $\alpha$, ensuring that the majority of values fall within the range $[0, 10]$.This sample-specific scaling avoids the limitations of global normalisation and makes the model more robust to differences in scale across the dataset.

Finally, all scaled values are rounded to two decimal places. This rounding step reduces the number of unique numeric tokens, improving tokenisation consistency and helping prevent overfitting to insignificant digit-level variation.


\subsection*{LLMTIME Encoding}

Following the LLMTIME convention, each timestep is represented as a pair of variables (prey, predator), separated by a comma. Consecutive timesteps are separated by a semicolon. For example, a sequence of three timesteps would be formatted as:
\begin{center}
\texttt{0.25,1.50;0.27,1.47;0.31,1.42}
\end{center}

While the original LLMTIME implementation by \citet{gruver2023language} removes decimal points to reduce sequence length—especially for GPT-style models that tokenize numbers into subword units, we retain the decimal point in our implementation for several important reasons:

\begin{itemize}
  \item Removing the decimal point would introduce additional digits and increase sequence length without benefit.
  \item Preserving the decimal enhances human interpretability and simplifies decoding during inference.
  \item The coursework specification explicitly recommends retaining the decimal point.
\end{itemize}

\subsection*{Tokenization}

Once formatted, the numeric string is passed through Qwen2.5’s tokenizer using Hugging Face’s \texttt{AutoTokenizer} interface. Each digit and punctuation mark is tokenized into its own token. For example:
\begin{verbatim}
tokenizer("1.23", return_tensors="pt")["input_ids"].tolist()[0]
\end{verbatim}
yields:
\begin{verbatim}
[16, 13, 17, 18]
\end{verbatim}

\paragraph{Example 1.}
\begin{itemize}
  \item \textbf{Original Input:}
  \begin{verbatim}
Prey:     [2.9, 3.2, 3.8, 4.5, 5.1]
Predator: [1.1, 0.9, 0.7, 0.6, 0.5]
  \end{verbatim}

  \vspace{-0.7cm}

  \item \textbf{Scale Factor $\alpha$:} 0.498


  \item \textbf{Formatted Sequence:}
  \begin{verbatim}
5.82,2.21;6.43,1.81;7.63,1.41;9.04,1.20;10.24,1.00
  \end{verbatim}

  \vspace{-0.7cm}

  \item \textbf{Tokenized Output (Qwen2.5 input IDs):}
  \begin{verbatim}
[20, 13, 23, 17, 11, 17, 13, 17, 16, 26, 21, 13, 19, 18, 11,
16, 13, 23, 16, 26, 22, 13, 21, 18, 11, 16, 13, 19, 16, 26,
24, 13, 15, 19, 11, 16, 13, 17, 15, 26, 16, 15, 13, 17, 19,
11, 16, 13, 15, 15]
  \end{verbatim}
\end{itemize}

\vspace{-1cm}

\paragraph{Example 2.}
\begin{itemize}
  \item \textbf{Original Input:}
  \begin{verbatim}
Prey:     [1.5, 1.8, 2.1, 2.4, 2.7]
Predator: [2.8, 2.5, 2.2, 1.9, 1.6]
  \end{verbatim}

  \vspace{-0.7cm}

  \item \textbf{Scale Factor $\alpha$:} 0.274

  \item \textbf{Formatted Sequence:}
  \begin{verbatim}
5.47,10.22;6.57,9.12;7.66,8.03;8.76,6.93;9.85,5.84
  \end{verbatim}

  \vspace{-0.7cm}

  \item \textbf{Tokenized Output (Qwen2.5 input IDs):}
  \begin{verbatim}
[20, 13, 19, 22, 11, 16, 15, 13, 17, 17, 26, 21, 13, 20, 22,
11, 24, 13, 16, 17, 26, 22, 13, 21, 21, 11, 23, 13, 15, 18,
26, 23, 13, 22, 21, 11, 21, 13, 24, 18, 26, 24, 13, 23, 20,
11, 20, 13, 23, 19]
  \end{verbatim}
\end{itemize}

\begin{figure}[H]
  \centering

  \begin{subfigure}[t]{0.95\textwidth}
      \centering
      \includegraphics[width=\textwidth]{true_vs_decoded.png}
      \caption{Decoded output vs.\ ground truth for the 972\textsuperscript{nd} Lotka-Volterra trajectory in the test set. This sequence was passed through the LLMTIME preprocessing pipeline and fed into the untrained Qwen2.5-Instruct model. The resulting output was decoded and rescaled using the same scale factor $\alpha$ applied during encoding. The close alignment between original and decoded values confirms that the preprocessing, tokenization, and decoding pipeline is working as intended.}
      \label{fig:true_vs_decoded}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}[t]{0.95\textwidth}
      \centering
      \includegraphics[width=\textwidth]{absolute_differences.png}
      \caption{Absolute differences between true and decoded prey (left) and predator (right) values for the 972\textsuperscript{nd} test sequence. These errors quantify the deviation introduced by the LLMTIME encoding-decoding pipeline. The low magnitude of differences confirms the correctness of the numeric formatting, tokenizer compatibility, and inverse decoding process, independent of model training.}
      \label{fig:absolute_differences}
  \end{subfigure}
  \vspace{0.5cm}
  \caption{ Encoding - Decoding check for Sample 972.}
  \label{fig:decoding_pipeline_evaluation}
\end{figure}

  
\section{Baseline Evaluation}

We assessed the forecasting ability of the untrained \texttt{Qwen2.5-0.5B-Instruct} model using Sample ID 972. The first 50 timesteps (prey–predator pairs) were provided as input in LLMTIME format, and the model was tasked with generating the remaining 50 steps.


Using the Hugging Face \texttt{generate()} API, the model predicted one token at a time, with each new token requiring a full forward pass. The 50-step input tokenized to \textbf{499 tokens}.

  \subsection*{Forecasting Performance}
  
  We compare the predicted output (post-decoding and re-scaling) to the true population values for Sample ID 972 using standard metrics:
  
  \begin{itemize}
    \item \textbf{Prey:}
      \begin{itemize}
        \item Mean Squared Error (MSE): 0.1770
        \item Mean Absolute Error (MAE): 0.2485
        \item $R^2$ Score: -0.2812
      \end{itemize}
    \item \textbf{Predator:}
      \begin{itemize}
        \item Mean Squared Error (MSE): 0.2337
        \item Mean Absolute Error (MAE): 0.2793
        \item $R^2$ Score: -1.6260
      \end{itemize}
  \end{itemize}
  
  The prey trajectory shows moderate alignment, albeit with oversmoothing. The predator prediction, however, is unstable, with a negative $R^2$ indicating performance worse than simply predicting the mean. This reflects the model’s lack of understanding of the underlying dynamics without domain-specific training.
  
 
  \begin{figure}[h]
      \centering
      \includegraphics[width=0.9\textwidth]{sample972_untrained.png}
      \caption{Forecasting output of the untrained Qwen2.5-Instruct model on Sample ID 972. The model is prompted with the first 50 timesteps and generates the next 50. Dashed lines show true values; solid lines show decoded predictions.}
      \label{fig:sample972_untrained}
  \end{figure}
  
  The untrained Qwen2.5-Instruct model exhibits some capacity to mimic the structure of the prey trajectory under LLMTIME formatting, likely due to its strong inductive bias and learned text patterns. However, it struggles with the more complex predator dynamics, which results in erratic predictions. This reinforces the need for fine-tuning or domain-specific adaptation when applying language models to structured forecasting problems.
  
  \section{FLOP Model}

To estimate the total FLOPs used by the Qwen2.5-0.5B-Instruct model, we mapped every major component in the forward pass to arithmetic operations and computed their FLOP costs using Table~\ref{tab:flops_primitives}. Bias additions are ignored throughout, as they are included in the matrix multiplication term. For inference, we assume reuse of Key/Value caches, meaning dot-product attention scores are only computed for new tokens, not the full input sequence.

\subsection*{Token and Positional Embeddings}
\begin{itemize}
  \item \textbf{Token Embeddings:} Retrieved via table lookup — \textbf{0 FLOPs}.
  \item \textbf{Positional Embedding Addition:} $n \times d_{\text{model}}$ additions.
\end{itemize}

\subsection*{RMSNorm (applied before attention, after attention, and before MLP)}

The Root Mean Square Layer Normalisation (RMSNorm) \citep{zhang2019root} normalises each input token $x \in \mathbb{R}^d$ using the root mean square of its elements, without subtracting the mean. The operation is defined as:
\begin{equation}
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
\end{equation}
where $\gamma \in \mathbb{R}^d$ is a learned scaling parameter and $\epsilon$ is a small constant for numerical stability.

For a batch of $n$ tokens (each of dimension $d$), the FLOPs are as follows:
\begin{itemize}
  \item Square each element: $n \times d$ multiplications.
  \item Sum across dimensions: $n \times (d - 1)$ additions.
  \item Compute the square root of the mean: $n$ square root operations.
  \item Divide each element by the norm: $n \times d$ divisions.
  \item Scale with learned weight $\gamma$: $n \times d$ multiplications.
\end{itemize}

\textbf{Total:} $2nd$ multiplications, $n(d - 1)$ additions, $nd$ divisions, $n$ square roots.


\subsection*{Multi-Head Attention (per layer)}
Let $h$ be the number of heads and $d_h = d / h$.
\begin{itemize}
  \item \textbf{Q/K/V Projections:} $3 \times n \times d \times d_h$ multiplications and $3 \times n \times (d - 1) \times d_h$ additions.
  \item \textbf{Attention Scores:} $n \times n \times d_h$ multiplications and $n \times n \times (d_h - 1)$ additions.
  \item \textbf{Softmax:} For $n^2$ elements:
    \begin{itemize}
      \item Exponentiation: $n^2$
      \item Summation: $n \times (n - 1)$ additions
      \item Normalisation: $n^2$ divisions
    \end{itemize}
  \item \textbf{Softmax-Value Multiplication:} $n \times n \times d_h$ multiplications and $n \times d_h \times (n - 1)$ additions.
  \item \textbf{Concatenation:} Memory operation — \textbf{0 FLOPs}.
  \item \textbf{Final Output Projection:} $n \times d \times d$ multiplications and $n \times (d - 1) \times d$ additions.
\end{itemize}
\textit{Note: For inference, we reuse cached Key and Value projections from previous tokens. Only new tokens incur the full attention computation.}

\subsection*{MLP Block with SwiGLU (per layer)}

Let $d$ be the model embedding dimension, $d_{\text{ff}}$ the hidden (feedforward) dimension (e.g., 4864), and $n$ the number of tokens in the sequence. The MLP block consists of two parallel up-projections (one gated) and a down-projection, followed by a SwiGLU activation \citep{shazeer2020glu}.

\begin{itemize}
  \item \textbf{Up-Projections and Gating:} Two parallel linear layers (one for gate, one for activation) each compute:
  \[
  \text{Multiplications: } n \times d \times d_{\text{ff}}, \quad
  \text{Additions: } n \times (d - 1) \times d_{\text{ff}}
  \]
  Multiplied by 2 for both paths:
  \[
  \Rightarrow 2 \times n \times d \times d_{\text{ff}} \text{ multiplications, and } 2 \times n \times (d - 1) \times d_{\text{ff}} \text{ additions.}
  \]

  \item \textbf{SwiGLU Activation:} The gated activation combines SiLU and elementwise multiplication:
  \begin{itemize}
    \item Each unit requires: 1 exponentiation, 1 division, 2 multiplications, and 1 addition.
    \item Total per token: $n \times d_{\text{ff}}$ of each (add, div, exp) and $2 \times n \times d_{\text{ff}}$ multiplications.
  \end{itemize}

  \item \textbf{Down-Projection:} A single linear layer reduces dimensionality:
  \[
  \text{Multiplications: } n \times d_{\text{ff}} \times d, \quad
  \text{Additions: } n \times (d_{\text{ff}} - 1) \times d
  \]
\end{itemize}


\subsection*{Final Projection to Vocabulary Logits}
\begin{itemize}
  \item \textbf{Linear Projection:} $n \times d \times V$ multiplications and $n \times (d - 1) \times V$ additions.
\end{itemize}

\subsection*{LoRA Projections (if enabled)}

Low-Rank Adaptation (LoRA) replaces a standard linear layer $W \in \mathbb{R}^{d \times d}$ with a low-rank approximation consisting of two smaller matrices:
\begin{itemize}
  \item A \textbf{down-projection} matrix $A \in \mathbb{R}^{r \times d}$ (reducing dimensionality),
  \item An \textbf{up-projection} matrix $B \in \mathbb{R}^{d \times r}$ (projecting back to the original space).
\end{itemize}

Let:
\begin{itemize}
  \item $n$ be the number of tokens in the input sequence,
  \item $d$ be the model's hidden dimension,
  \item $r$ be the LoRA rank.
\end{itemize}

\begin{itemize}
  \item \textbf{Down-Projection ($A x$):} Projects from $\mathbb{R}^{d}$ to $\mathbb{R}^{r}$:
  \begin{itemize}
    \item Multiplications: $n \times d \times r$
    \item Additions: $n \times (d - 1) \times r$
  \end{itemize}

  \item \textbf{Up-Projection ($B (A x)$):} Projects from $\mathbb{R}^{r}$ back to $\mathbb{R}^{d}$:
  \begin{itemize}
    \item Multiplications: $n \times r \times d$
    \item Additions: $n \times (r - 1) \times d$
  \end{itemize}

  \item \textbf{Scaling and Residual Addition:} The result is scaled (typically by $\alpha / r$) and added to the original output:
  \begin{itemize}
    \item Multiplications: $n \times d$
    \item Additions: $n \times d$
  \end{itemize}
\end{itemize}

\textbf{Total FLOPs for one LoRA adapter:}
\begin{itemize}
  \item \textbf{Multiplications:} $2 \times n \times d \times r + n \times d$
  \item \textbf{Additions:} $2 \times n \times d \times r + n \times d$ (approximately)
\end{itemize}
This total assumes that both the down and up projections are active and used once per token per transformer block.


\subsection*{Summary}

The total FLOPs are computed as:
\[
\text{Total FLOPs} = \sum_{i=1}^{n} \left( a_i + m_i + d_i + 10e_i + 10s_i \right)
\]
where $a_i$, $m_i$, $d_i$, $e_i$, and $s_i$ represent the counts of additions, multiplications, divisions, exponentiations, and square roots respectively.

All computations are implemented in the Python function \texttt{forwards\_pass\_flops()}, which returns detailed per-operation and total FLOPs based on sequence length, LoRA rank, and model configuration.


\section{LoRA Adaptation and Fine-Tuning Procedure}

To enable parameter-efficient fine-tuning of the Qwen2.5-0.5B-Instruct model, we applied \textbf{Low-Rank Adaptation (LoRA)} to the \textbf{query (Q)} and \textbf{value (V)} projection layers in each transformer block. Specifically, we wrapped each of these layers with a custom \texttt{LoRALinear} module that augments the frozen base projection with a trainable low-rank update.

For each modified projection layer, we introduced two trainable matrices:
\begin{itemize}
    \item A down-projection matrix $A \in \mathbb{R}^{r \times d}$
    \item An up-projection matrix $B \in \mathbb{R}^{d \times r}$
\end{itemize}

The output of a LoRA-adapted linear layer becomes:
\[
\text{output} = W x + \frac{\alpha}{r} B A x
\]
where $W$ is the original frozen weight matrix and $\alpha$ is a scaling factor (typically set equal to $r$) \citep{hu2021lora}.

We fine-tuned only the LoRA parameters ($A$, $B$) while keeping the base model parameters frozen. The adapted layers were implemented by replacing \texttt{q\_proj} and \texttt{v\_proj} in each transformer block:

Training was performed for 600 steps using default hyperparameters:
\begin{itemize}
    \item \texttt{lora\_rank} = 4
    \item \texttt{learning\_rate} = 1e-5
    \item \texttt{batch\_size} = 4
    \item \texttt{context\_length} = 512
\end{itemize}

We compared the trained model against an untrained baseline (LoRA-enabled but with zero optimisation steps). Both models were evaluated on a held-out validation set of 200 trajectories, using cross-entropy loss as the evaluation metric.

\subsection*{Results}

\begin{itemize}
    \item \textbf{Untrained model (LoRA only, 0 steps):} Validation loss = \textbf{1.1926}
    \item \textbf{LoRA-trained model (600 steps):} Validation loss = \textbf{0.9555}
\end{itemize}

The performance improvement demonstrates that even a modest number of training steps is sufficient to adapt the model to the domain of Lotka–Volterra trajectories.

\section{LoRA Hyperparameter Search}

To understand how key hyperparameters affect forecasting performance, we carried out a targeted grid search over:
\begin{itemize}
    \item \textbf{LoRA Rank} $\in \{2, 4, 8\}$
    \item \textbf{Learning Rate} $\in \{10^{-5}, 5 \times 10^{-5}, 10^{-4}\}$
\end{itemize}

This search was designed to explore the trade-off between model expressivity and training dynamics. Increasing the LoRA rank introduces more trainable parameters per adapted layer, improving flexibility but increasing FLOP usage. Conversely, the learning rate controls the speed of adaptation: values that are too high may destabilise training, while those too low may cause underfitting or slow convergence.

Each configuration was trained for 600 optimiser steps and evaluated on a held-out validation set. Due to FLOP constraints and clear trends in performance, not all combinations were run. Specifically, validation loss consistently decreased as we moved \textbf{downward} (higher LoRA rank) and \textbf{leftward} (higher learning rate) in the grid. These trends allowed us to avoid redundant runs while still identifying the most promising setup.

\vspace{0.2cm}

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
      \hline
      \textbf{LoRA Rank} & \textbf{LR = $10^{-4}$} & \textbf{LR = $5 \times 10^{-5}$} & \textbf{LR = $10^{-5}$} \\
      \hline
      2 & 
      \makecell{0 steps \\ 0.0000\% \\ n/a} & 
      \makecell{600 steps \\ 4.2212\% \\ 0.7899} & 
      \makecell{0 steps \\ 0.0000\% \\ n/a} \\
      \hline
      4 & 
      \makecell{600 steps \\ 4.22252\% \\ 0.6551} & 
      \makecell{600 steps \\ 4.22252\% \\ 0.7444} & 
      \makecell{600 steps \\ 4.22252\% \\ 0.9555} \\
      \hline
      8 & 
      \makecell{600 steps \\ 4.22516\% \\ 0.6147} & 
      \makecell{600 steps \\ 4.22516\% \\ 0.6788} & 
      \makecell{0 steps \\ 0.0000\% \\ n/a} \\
      \hline
  \end{tabular}
  \vspace{0.2cm}
  \caption{LoRA Hyperparameter Search Results. Each cell contains: training steps (top), percentage of FLOPs used (middle), and validation loss (bottom). Results for LoRA rank=4 and LR=1e-5 were reused from the previous section}
  \label{tab:lora_grid_search}
\end{table}

As shown in Table~\ref{tab:lora_grid_search}, the best-performing configuration used \textbf{LoRA rank 8} and a \textbf{learning rate of $10^{-4}$}, achieving a validation loss of \textbf{0.6147} with only \textbf{4.23\%} of the total FLOP budget. This combination strikes an effective balance between adaptation capacity and compute efficiency.

\subsection*{Effect of Context Length on Forecasting Performance}

Having identified an effective LoRA configuration, we next explored how the model’s performance is influenced by \textbf{context length}—the number of input tokens the model sees at once. Using the same LoRA rank (8) and learning rate ($10^{-4}$), we trained models using context lengths of \{128, 512, 768\}:

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Context Length} & \textbf{Validation Loss} & \textbf{Optimiser Steps} & \textbf{FLOPs Used (\%)} \\
    \hline
    128 & 0.7088 & 600 & 1.02421 \\
    512 & 0.6147 & 600 & \textit{(from previous section)} 4.22516 \\
    768 & 0.4207 & 600 & 6.46607 \\
    \hline
  \end{tabular}
  \vspace{0.2cm}
  \caption{Effect of context length on validation loss using LoRA rank 8 and learning rate $10^{-4}$.}
  \label{tab:context_length_results}
\end{table}

As expected, validation loss improves with increasing context length. This trend is intuitive: longer contexts provide the model with more of the input sequence, enabling it to learn richer temporal patterns—especially important in cyclical systems like the Lotka–Volterra dynamics.

Given that Qwen2.5-Instruct is an autoregressive decoder-only model, its predictions are entirely conditioned on previous tokens. Extending the context window increases the number of prey–predator pairs the model can attend to during forecasting, resulting in more accurate extrapolations.

However, this benefit comes at a significant computational cost. Moving from 128 to 768 tokens increases the proportion of total FLOPs used from approximately 1\% to over ~ 6\%, even with the same number of training steps.


\section{Final Model Training and Evaluation}

Based on insights from the previous experiments, we selected the following hyperparameters for our final model configuration:
\begin{itemize}
    \item \textbf{LoRA Rank:} 8
    \item \textbf{Learning Rate:} $10^{-4}$
    \item \textbf{Context Length:} 768
\end{itemize}

This configuration was chosen as it consistently outperformed others during the hyperparameter and context length sweeps, offering a strong balance between predictive accuracy and FLOP efficiency.

We trained the model for 6,000 steps using this setup, which approximately 62\% of the total FLOP budget. Evaluation on a held-out validation set yielded a final loss of 0.2878 and a perplexity of 1.33. Figure~\ref{fig:training_loss} shows the full training curve, while Figure~\ref{fig:zoomed_training_loss} presents a zoomed-in view of the final 3,000 steps.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{training_loss.png}
    \caption{Training loss curve for the full 6,000-step run. The dashed red line indicates final validation loss.}
    \label{fig:training_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{zoomed_training_loss.png}
    \caption{Training loss over the last 3,000 steps. The model shows stable convergence.}
    \label{fig:zoomed_training_loss}
\end{figure}

The validation loss is comparable to the training loss throughout, suggesting that the model generalises well to unseen data and is not overfitting.

\subsection*{Evaluation}

To assess forecasting performance, we used the trained model to generate predictions on a representative test sequence (Sample ID 972). As shown in Figure~\ref{fig:sample_prediction}, the model closely tracks the ground-truth oscillatory behaviour of both prey and predator populations across the full 100-timestep horizon.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{sample972_trained.png}
    \caption{Forecasting results on Sample 972. The model successfully captures oscillatory dynamics in both prey (left) and predator (right) populations.}
    \label{fig:sample_prediction}
\end{figure}

\begin{table}[H]
    \centering

    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Species} & \textbf{MSE} & \textbf{MAE} & \textbf{R$^2$} \\
        \hline
        Prey & 0.0170 & 0.0671 & 0.8772 \\
        Predator & 0.0125 & 0.0608 & 0.8600 \\
        \hline
    \end{tabular}
    \vspace{0.2cm}
    \caption{Evaluation metrics for Sample 972}
    \label{tab:metrics_sample972}
\end{table}

\section{Discussion + Assumptions}

In this coursework, we assume a compute efficiency of 1, meaning all estimated FLOPs are fully utilised during inference without memory or runtime optimisations. Some large-scale language models, such as GPT-4, employ Mixture-of-Experts (MoE) architectures to reduce active parameters per forward pass—this lowers compute cost by activating only a subset of layers during token generation. However, \texttt{Qwen2.5-0.5B-Instruct} does not employ MoE or dynamic routing, meaning that \textbf{all model parameters are active for every forward pass}.

Recent innovations such as SkipDecode~\cite{delcorro2023skipdecode} propose early-exit strategies that skip computation in lower layers during autoregressive decoding. This approach can significantly reduce inference FLOPs while preserving performance, and is compatible with batching and KV cache reuse. Nonetheless, \texttt{Qwen2.5-0.5B-Instruct} does not implement such mechanisms, and so the full model is evaluated on every token without layer skipping or selective computation.

It is important to note that generation is inherently non-deterministic unless a seed is set. This means different runs on the same input can yield very different outputs—some closer to ground truth, others worse. This behaviour arises from sampling strategies like temperature scaling or nucleus sampling used during decoding.


\section{FLOPs Usage}

insert table for all flops used






\bibliographystyle{unsrtnat}
\bibliography{references}


\end{document}
